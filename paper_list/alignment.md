# Alignment

## Papers

### 2023

- (2023-08) **Aligning Large Language Models with Human: A Survey** [paper](https://arxiv.org/abs/2307.12966)
- (2023-05) **LIMA: Less Is More for Alignment** [paper](https://arxiv.org/abs/2305.11206)

- (2023-05) **RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs** [paper](https://arxiv.org/abs/2305.08844)

- (2023-05) **Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision** [paper](https://arxiv.org/abs/2305.03047)

- (2023-05) **Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback** [paper](https://arxiv.org/abs/2305.10142)

- (2023-04) **Fundamental Limitations of Alignment in Large Language Models** [paper](https://arxiv.org/abs/2304.11082)

## Useful Resources
- [Awesome-Align-LLM-Human](https://github.com/GaryYufei/AlignLLMHumanSurvey) - A collection of papers and resources about aligning large language models (LLMs) with human.

